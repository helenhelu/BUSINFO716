{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_yN-wcvzaX0",
        "outputId": "5134a994-587d-4657-85d5-5ba5ce39ece8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.0 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iBSACYDvy9rS"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classify statments\n",
        "\n",
        "#text = 'Emissions have increased during the last several months'\n",
        "#text = \"Air New Zealand’s employee engagement score being in Glint's Global Top 20% Engagement Index.\"\n",
        "#text = \"There can be few people who now doubt that accelerating climate change represents a massive challenge for governments and businesses the world over. But not many yet understand the true nature of this crisis, with disruption from more and more climate-induced disasters becoming ever more deadly – and costly.\" \n",
        "#text = \"Establish a baseline of Air New Zealand spend with Māori and Pasifika-owned businesses and social enterprises by 2022.\"\n",
        "text = \"Hydro has also started working on several initiatives to reduce direct CO2 emission in primary aluminium production.\"\n",
        "#text = 'this is an annual sustainability report'"
      ],
      "metadata": {
        "id": "lOQ7LCGS0u8K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify whether a text is related to climate topics.\n",
        "Varini, F. S., Boyd-Graber, J., Ciaramita, M., & Leippold, M. (2020). \n",
        "ClimaText: A dataset for climate change topic detection. arXiv preprint arXiv:2012.00483.\n",
        "\n",
        "Webersinke, N., Kraus, M., Bingler, J. A., & Leippold, M. (2021). \n",
        "Climatebert: A pretrained language model for climate-related text. arXiv preprint arXiv:2110.12010.\n",
        "\n",
        "------------------------------\n",
        "https://kruthof.github.io"
      ],
      "metadata": {
        "id": "7-bdWDoi5ho9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline,RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-f\")\n",
        "climateattention = RobertaForSequenceClassification.from_pretrained('kruthof/climateattention-ctw',num_labels=2)\n",
        "\n",
        "ClimateAttention = pipeline(\"text-classification\", model=climateattention, tokenizer=tokenizer)\n",
        "\n",
        "ClimateAttention(text)\n"
      ],
      "metadata": {
        "id": "5WGKrL_jxfXY",
        "outputId": "50a0fa37-f818-4ec1-9d16-6389dece5df1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'Yes', 'score': 0.9994627833366394}]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify whether a text is related to Environmental, Social or Governance\n",
        "Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" Contemporary Accounting Research (2022).\n",
        "[Huggingface link](https://huggingface.co/yiyanghkust/finbert-esg)"
      ],
      "metadata": {
        "id": "ijqi0bfB4Dra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tested in transformers==4.18.0 \n",
        "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')\n",
        "classifier_esg = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n",
        "results = classifier_esg(text)\n",
        "print(results) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBQX-Pte1_-U",
        "outputId": "ab52cdf2-6281-46ff-cbb5-a0a1542a4859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'Environmental', 'score': 0.9935469031333923}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify whether a text is an environmental claim.\n",
        "\n",
        "Stammbach, D., Webersinke, N., Bingler, J. A., Kraus, M., & Leippold, M. (2022). A Dataset for Detecting Real-World Environmental Claims. arXiv preprint arXiv:2209.00507.\n",
        "\n",
        "\"For constructing the dataset, we were inspired by the European Commission (EC), which defines such claims as follows: Environmental claims refer to the\n",
        "practice of suggesting or otherwise creating the impression (in the context of a commercial communication, marketing or advertising) that a product or a service is environmentally friendly (i.e., it has a positive impact on the environment) or is less damaging to the environment than competing goods or services.\""
      ],
      "metadata": {
        "id": "cZRejwbB6mwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'climatebert/environmental-claims'\n",
        "model = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer = pipeline('text-classification', model=model_name, tokenizer=model_name)\n",
        "\n",
        "output = tokenizer(text)\n",
        "pred = output[0]['label']\n",
        "print(\"Prediction:\", pred)\n"
      ],
      "metadata": {
        "id": "TeH3O4llp3q6",
        "outputId": "fd3c1f4d-08c3-4771-a489-127a40c88594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis (To Classify whether it is positive, neutral or negative)"
      ],
      "metadata": {
        "id": "M5H8wixQ7bSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = pipeline(\"sentiment-analysis\", model=\"facebook/bart-large-mnli\")\n",
        "sentiment(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEElTKwY0kfT",
        "outputId": "9c3e475e-33e4-4560-f40f-83960a917bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'neutral', 'score': 0.9972754120826721}]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}